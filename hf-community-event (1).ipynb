{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install git+https://github.com/huggingface/transformers.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-19T07:06:04.109739Z","iopub.execute_input":"2021-11-19T07:06:04.110363Z","iopub.status.idle":"2021-11-19T07:06:40.390833Z","shell.execute_reply.started":"2021-11-19T07:06:04.110270Z","shell.execute_reply":"2021-11-19T07:06:40.390009Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/rathiankit03/ImageCaptionHindi/master/Flickr8kHindiDataset/Flickr8k-Hindi.txt","metadata":{"execution":{"iopub.status.busy":"2021-11-19T07:06:40.394401Z","iopub.execute_input":"2021-11-19T07:06:40.394619Z","iopub.status.idle":"2021-11-19T07:06:42.209644Z","shell.execute_reply.started":"2021-11-19T07:06:40.394591Z","shell.execute_reply":"2021-11-19T07:06:42.208832Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T07:06:42.211321Z","iopub.execute_input":"2021-11-19T07:06:42.211617Z","iopub.status.idle":"2021-11-19T07:06:43.678643Z","shell.execute_reply.started":"2021-11-19T07:06:42.211581Z","shell.execute_reply":"2021-11-19T07:06:43.677892Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nbase_path = '../input/flickr8k/Images/'\nwith open('./Flickr8k-Hindi.txt') as f:\n    data = []\n    \n    for i in f.readlines():\n        sp = i.split(' ')\n        data.append([sp[0] + '.jpg', ' '.join(sp[1:])])\n        \nhindi = pd.DataFrame(data, columns = ['images', 'text'])\n#hindi['images'] = hindi['images']!='2258277193_58694969e2'\nhindi.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T07:06:43.680607Z","iopub.execute_input":"2021-11-19T07:06:43.681045Z","iopub.status.idle":"2021-11-19T07:06:43.831185Z","shell.execute_reply.started":"2021-11-19T07:06:43.681006Z","shell.execute_reply":"2021-11-19T07:06:43.830434Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"hindi = hindi[hindi['images']!='2258277193_58694969e2']\nhindi","metadata":{"execution":{"iopub.status.busy":"2021-11-19T07:06:43.832579Z","iopub.execute_input":"2021-11-19T07:06:43.833014Z","iopub.status.idle":"2021-11-19T07:06:43.855242Z","shell.execute_reply.started":"2021-11-19T07:06:43.832976Z","shell.execute_reply":"2021-11-19T07:06:43.854572Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(hindi, test_size=0.2)\n# we reset the indices to start from zero\ntrain_df.reset_index(drop=True, inplace=True)\ntest_df.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T07:06:43.856509Z","iopub.execute_input":"2021-11-19T07:06:43.856764Z","iopub.status.idle":"2021-11-19T07:06:44.695105Z","shell.execute_reply.started":"2021-11-19T07:06:43.856731Z","shell.execute_reply":"2021-11-19T07:06:44.694367Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nclass Image_Caption_Dataset(Dataset):\n    def __init__(self,root_dir,df, feature_extractor,tokenizer,max_target_length=128):\n        self.root_dir = root_dir\n        self.df = df\n        self.feature_extractor = feature_extractor\n        self.tokenizer = tokenizer\n        self.max_length=max_target_length\n        \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self,idx):\n        #return image\n        image_path = self.df['images'][idx]\n        text = self.df['text'][idx]\n        #prepare image\n        image = Image.open(self.root_dir+'/'+image_path).convert(\"RGB\")\n        pixel_values = self.feature_extractor(image, return_tensors=\"pt\").pixel_values\n        #add captions by encoding the input\n        captions = self.tokenizer(text,\n                                 padding='max_length',\n                                 max_length=self.max_length).input_ids\n        captions = [caption if caption != self.tokenizer.pad_token_id else -100 for caption in captions]\n        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(captions)}\n        return encoding","metadata":{"execution":{"iopub.status.busy":"2021-11-19T07:08:29.368799Z","iopub.execute_input":"2021-11-19T07:08:29.369578Z","iopub.status.idle":"2021-11-19T07:08:29.378202Z","shell.execute_reply.started":"2021-11-19T07:08:29.369537Z","shell.execute_reply":"2021-11-19T07:08:29.377321Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from transformers import ViTFeatureExtractor,AutoTokenizer\n\nencoder_checkpoint = 'google/vit-base-patch16-224'\ndecoder_checkpoint = 'surajp/gpt2-hindi'\n\nfeature_extractor = ViTFeatureExtractor.from_pretrained(encoder_checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(decoder_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T07:08:30.375158Z","iopub.execute_input":"2021-11-19T07:08:30.375624Z","iopub.status.idle":"2021-11-19T07:08:35.645188Z","shell.execute_reply.started":"2021-11-19T07:08:30.375588Z","shell.execute_reply":"2021-11-19T07:08:35.644362Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"root_dir = \"../input/flickr8k/Images\"\n\n\ntrain_dataset = Image_Caption_Dataset(root_dir=root_dir,\n                           df=train_df,\n                           feature_extractor=feature_extractor,\n                                     tokenizer=tokenizer)\nval_dataset = Image_Caption_Dataset(root_dir=root_dir,\n                           df=test_df,\n                           feature_extractor=feature_extractor,\n                                     tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T07:08:35.646952Z","iopub.execute_input":"2021-11-19T07:08:35.647210Z","iopub.status.idle":"2021-11-19T07:08:35.653159Z","shell.execute_reply.started":"2021-11-19T07:08:35.647177Z","shell.execute_reply":"2021-11-19T07:08:35.651338Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"encoding = train_dataset[0]\nfor k,v in encoding.items():\n  print(k, v.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T07:08:35.654267Z","iopub.execute_input":"2021-11-19T07:08:35.656210Z","iopub.status.idle":"2021-11-19T07:08:35.684058Z","shell.execute_reply.started":"2021-11-19T07:08:35.656173Z","shell.execute_reply":"2021-11-19T07:08:35.683355Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"labels = encoding['labels']\nlabels[labels == -100] = tokenizer.pad_token_id\nlabel_str = tokenizer.decode(labels, skip_special_tokens=True)\nprint(label_str)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T07:08:35.685815Z","iopub.execute_input":"2021-11-19T07:08:35.686385Z","iopub.status.idle":"2021-11-19T07:08:35.692311Z","shell.execute_reply.started":"2021-11-19T07:08:35.686351Z","shell.execute_reply":"2021-11-19T07:08:35.691581Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\neval_dataloader = DataLoader(val_dataset, batch_size=4)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T07:08:39.391655Z","iopub.execute_input":"2021-11-19T07:08:39.391961Z","iopub.status.idle":"2021-11-19T07:08:39.397582Z","shell.execute_reply.started":"2021-11-19T07:08:39.391928Z","shell.execute_reply":"2021-11-19T07:08:39.396703Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from transformers import VisionEncoderDecoderModel\n# initialize a vit-bert from a pretrained ViT and a pretrained BERT model. Note that the cross-attention layers will be randomly initialized\nmodel = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_checkpoint, decoder_checkpoint)\n#model.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T07:08:51.553290Z","iopub.execute_input":"2021-11-19T07:08:51.553551Z","iopub.status.idle":"2021-11-19T07:09:29.680484Z","shell.execute_reply.started":"2021-11-19T07:08:51.553523Z","shell.execute_reply":"2021-11-19T07:09:29.679726Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# set special tokens used for creating the decoder_input_ids from the labels\nmodel.config.decoder_start_token_id = tokenizer.cls_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\n# make sure vocab size is set correctly\nmodel.config.vocab_size = model.config.decoder.vocab_size\n\n# set beam search parameters\nmodel.config.eos_token_id = tokenizer.sep_token_id\nmodel.config.max_length = 128\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4","metadata":{"execution":{"iopub.status.busy":"2021-11-19T07:13:26.023452Z","iopub.execute_input":"2021-11-19T07:13:26.023771Z","iopub.status.idle":"2021-11-19T07:13:26.039653Z","shell.execute_reply.started":"2021-11-19T07:13:26.023735Z","shell.execute_reply":"2021-11-19T07:13:26.038396Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from datasets import load_metric\n\nbleu_metric = load_metric(\"bleu\")","metadata":{"execution":{"iopub.status.busy":"2021-11-19T07:09:29.689224Z","iopub.execute_input":"2021-11-19T07:09:29.689618Z","iopub.status.idle":"2021-11-19T07:09:31.825295Z","shell.execute_reply.started":"2021-11-19T07:09:29.689580Z","shell.execute_reply":"2021-11-19T07:09:31.824504Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    labels_ids[labels_ids == -100] = tokenizer.tokenizer.pad_token_id\n    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n\n    bleu = bleu_metric.compute(predictions=pred_str, references=label_str)\n\n    return {\"bleu\": bleu}","metadata":{"execution":{"iopub.status.busy":"2021-11-19T07:09:31.827125Z","iopub.execute_input":"2021-11-19T07:09:31.827887Z","iopub.status.idle":"2021-11-19T07:09:31.833791Z","shell.execute_reply.started":"2021-11-19T07:09:31.827827Z","shell.execute_reply":"2021-11-19T07:09:31.833061Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    predict_with_generate=True,\n    evaluation_strategy=\"steps\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    fp16=True, \n    output_dir=\"./\",\n    logging_steps=2,\n    save_steps=1000,\n    eval_steps=200,\n)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T07:13:32.180041Z","iopub.execute_input":"2021-11-19T07:13:32.180495Z","iopub.status.idle":"2021-11-19T07:13:32.195018Z","shell.execute_reply.started":"2021-11-19T07:13:32.180456Z","shell.execute_reply":"2021-11-19T07:13:32.194163Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"from transformers import default_data_collator\n\n# instantiate trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=feature_extractor,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    data_collator=default_data_collator,\n)\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T07:13:32.881713Z","iopub.execute_input":"2021-11-19T07:13:32.882420Z","iopub.status.idle":"2021-11-19T07:13:33.142760Z","shell.execute_reply.started":"2021-11-19T07:13:32.882381Z","shell.execute_reply":"2021-11-19T07:13:33.141706Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW\nfrom tqdm.notebook import tqdm\n\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nfor epoch in range(2):  # loop over the dataset multiple times\n   # train\n   model.train()\n   train_loss = 0.0\n   for batch in tqdm(train_dataloader):\n      # get the inputs\n      for k,v in batch.items():\n        batch[k] = v.to(device)\n\n      # forward + backward + optimize\n      outputs = model(**batch)\n      loss = outputs.loss\n      loss.backward()\n      optimizer.step()\n      optimizer.zero_grad()\n\n      train_loss += loss.item()\n\n   print(f\"Loss after epoch {epoch}:\", train_loss/len(train_dataloader))\n    \n   # evaluate\n   model.eval()\n   valid_bleu = 0.0\n   with torch.no_grad():\n     for batch in tqdm(eval_dataloader):\n       # run batch generation\n       outputs = model.generate(batch[\"pixel_values\"].to(device))\n       # compute metrics\n       cer = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])\n       valid_bleu += cer \n\n   print(\"Validation BLEU:\", valid_bleu / len(eval_dataloader))\n\nmodel.save_pretrained(\".\")","metadata":{"execution":{"iopub.status.busy":"2021-11-19T05:15:13.308344Z","iopub.execute_input":"2021-11-19T05:15:13.308599Z","iopub.status.idle":"2021-11-19T05:15:13.314193Z","shell.execute_reply.started":"2021-11-19T05:15:13.308570Z","shell.execute_reply":"2021-11-19T05:15:13.313241Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-11-19T05:18:09.421680Z","iopub.execute_input":"2021-11-19T05:18:09.422272Z","iopub.status.idle":"2021-11-19T05:18:09.426510Z","shell.execute_reply.started":"2021-11-19T05:18:09.422234Z","shell.execute_reply":"2021-11-19T05:18:09.425619Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}