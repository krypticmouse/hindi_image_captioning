{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install transformers","metadata":{"execution":{"iopub.status.busy":"2021-11-19T01:41:06.783935Z","iopub.execute_input":"2021-11-19T01:41:06.784208Z","iopub.status.idle":"2021-11-19T01:41:17.829546Z","shell.execute_reply.started":"2021-11-19T01:41:06.784176Z","shell.execute_reply":"2021-11-19T01:41:17.828722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom collections import Counter\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nfrom torch import nn, optim\nfrom torchvision import transforms","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-19T01:30:30.758995Z","iopub.execute_input":"2021-11-19T01:30:30.759463Z","iopub.status.idle":"2021-11-19T01:30:30.763676Z","shell.execute_reply.started":"2021-11-19T01:30:30.759425Z","shell.execute_reply":"2021-11-19T01:30:30.763004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"base_path = '../input/flickr8k/Images/'\nwith open('/kaggle/input/hindi-caption/Flickr8k-Hindi.txt') as f:\n    data = []\n    \n    for i in f.readlines():\n        sp = i.split(' ')\n        data.append([sp[0] + '.jpg', ' '.join(sp[1:])])\n        \nhindi = pd.DataFrame(data, columns = ['images', 'text'])\nhindi.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T01:30:30.840857Z","iopub.execute_input":"2021-11-19T01:30:30.841251Z","iopub.status.idle":"2021-11-19T01:30:31.071157Z","shell.execute_reply.started":"2021-11-19T01:30:30.841222Z","shell.execute_reply":"2021-11-19T01:30:31.070327Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LM Scratch for Swin and EfficientNet","metadata":{}},{"cell_type":"code","source":"paragraphs = list(hindi['text'])\ntext = \" \".join(paragraphs)\nwords = text.split(\" \")\n\nclass Tokenizer:\n    def __init__(self, maxlen = 50):\n        self.vocab = Counter()\n        self.maxlen = maxlen\n\n    def build_vocab(self, texts):\n        for sent in texts:\n            self.vocab.update(sent.split(' '))\n        \n        v,k = {'_pad_': 0, '_unk_': 1}, 2\n\n        for i in sorted(self.vocab, key = self.vocab.get, reverse = True): \n            if i == '_pad_':\n                continue\n            v[i] = k\n            k += 1\n        self.vocab = v\n        self.idx2word = {v[k]:k for k in v}\n\n    def __call__(self, text):\n        ans = []\n        l = [(self.vocab[j] if j in self.vocab else 1) for j in text.split()]\n        if len(l) >= self.maxlen:\n            return l[:self.maxlen]\n        else:\n            l.extend([0]*(self.maxlen - len(l)))\n            return l","metadata":{"execution":{"iopub.status.busy":"2021-11-19T00:24:44.614068Z","iopub.execute_input":"2021-11-19T00:24:44.614364Z","iopub.status.idle":"2021-11-19T00:24:44.686626Z","shell.execute_reply.started":"2021-11-19T00:24:44.61432Z","shell.execute_reply":"2021-11-19T00:24:44.685598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CaptionDataset:\n    def __init__(self, df, tranform):\n        self.df = df\n        self.transform = tranform\n        \n        self.tokenizer = Tokenizer(32)\n        self.tokenizer.build_vocab(df['text'])\n    \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, idx):\n        img_path = self.df.loc[idx, 'images']\n        \n        img = Image.open('../input/flickr8k/Images/' + img_path)\n        img = self.transform(img)\n        \n        caption = self.tokenizer(self.df.loc[idx, 'text'])\n        \n        return {\n            'image': torch.tensor(img),\n            'text': torch.tensor(caption, dtype = torch.long)\n        }","metadata":{"execution":{"iopub.status.busy":"2021-11-19T00:24:44.688182Z","iopub.execute_input":"2021-11-19T00:24:44.688692Z","iopub.status.idle":"2021-11-19T00:24:44.697697Z","shell.execute_reply.started":"2021-11-19T00:24:44.688653Z","shell.execute_reply":"2021-11-19T00:24:44.696941Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, random_split\n\ntransform = transforms.Compose([\n                transforms.Resize((224,224)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ndataset = CaptionDataset(hindi, transform)\n\ntrain_data, test_data = random_split(dataset, [32455,8000])\nvalid_data, test_data = random_split(test_data, [4000,4000])\n\ntrainloader = DataLoader(train_data, batch_size = 8)\nvalidloader = DataLoader(valid_data, batch_size = 8)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T00:24:44.700153Z","iopub.execute_input":"2021-11-19T00:24:44.700783Z","iopub.status.idle":"2021-11-19T00:24:44.904135Z","shell.execute_reply.started":"2021-11-19T00:24:44.700745Z","shell.execute_reply":"2021-11-19T00:24:44.903415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import ViTModel\nimport torch.nn.functional as F\n\nclass Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.img_model = ViTModel.from_pretrained('google/vit-base-patch16-224', add_pooling_layer=False)\n        self.features = nn.Linear(768,768)\n        self.dropout = nn.Dropout(0.3)\n    \n    def forward(self, image):\n        x = torch.mean(self.img_model(image).last_hidden_state, axis = 1)\n        x = self.dropout(F.relu(self.features(x)))\n        return x\n    \nclass Decoder(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.embed = nn.Embedding(vocab_size, 768)\n        self.lstm = nn.LSTM(768, 1024, num_layers=1)\n        self.out = nn.Linear(1024, vocab_size)\n        self.dropout = nn.Dropout(0.5)\n\n    def forward(self, features, text):\n        embeds = self.dropout(self.embed(text))\n        print(embeds.size(), features.size())\n        embeds = torch.cat((features.unsqueeze(0), embeds.permute(1,0,2)), dim=0)\n        hc, _ = self.lstm(embeds)\n        outputs = self.out(hc)\n        return outputs\n\nclass CaptionModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.encoder = Encoder()\n        self.decoder = Decoder(vocab_size)\n\n    def forward(self, images, text):\n        features = self.encoder(images)\n        outputs = self.decoder(features, text)\n        return outputs\n    \n    def generate(self, image, vocab, max_length=50):\n        result_caption = []\n\n        with torch.no_grad():\n            x = self.encoder(image).unsqueeze(0)\n            states = None\n\n            for _ in range(max_length):\n                hiddens, states = self.decoder.lstm(x, states)\n                output = self.decoder.linear(hiddens.squeeze(0))\n                predicted = output.argmax(1)\n                result_caption.append(predicted.item())\n                x = self.decoder.embed(predicted).unsqueeze(0)\n\n                if vocabulary.itos[predicted.item()] == \"_pad_\":\n                    break\n\n        return [vocabulary.idx2word[idx] for idx in result_caption]\n    \nmodel = CaptionModel(len(dataset.tokenizer.vocab)).cuda()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T00:29:08.2612Z","iopub.execute_input":"2021-11-19T00:29:08.261547Z","iopub.status.idle":"2021-11-19T00:29:10.184125Z","shell.execute_reply.started":"2021-11-19T00:29:08.261511Z","shell.execute_reply":"2021-11-19T00:29:10.183408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(ignore_index = 0)\noptimizer = optim.Adam(model.parameters())","metadata":{"execution":{"iopub.status.busy":"2021-11-19T00:29:10.18679Z","iopub.execute_input":"2021-11-19T00:29:10.187284Z","iopub.status.idle":"2021-11-19T00:29:10.192833Z","shell.execute_reply.started":"2021-11-19T00:29:10.187245Z","shell.execute_reply":"2021-11-19T00:29:10.192133Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\n\nepochs = 5\nmin_valid_loss = np.inf\n\nfor e in range(epochs):\n    model.train()\n    train_loss = 0.0\n    \n    for batch in tqdm(trainloader):\n        optimizer.zero_grad(set_to_none = True)\n        img, text = batch['image'].cuda(), batch['text'].cuda()\n        \n        pred = model(img, text)[:-1]\n        loss = criterion(pred.reshape(-1, pred.shape[2]), text.reshape(-1))\n        train_loss = 0.0\n        \n        loss.backward()\n        optimizer.step()\n        \n    valid_loss = 0.0\n    with torch.no_grad():\n        model.eval()\n        for batch in tqdm(validloader):\n            img, text = batch['image'].cuda(), batch['text'].cuda()\n            \n            pred = model(img, text)\n            loss = criterion(pred.reshape(-1, pred.shape[2]), text.reshape(-1))\n            valid_loss += 0\n            \n    print(f'Training Loss:{train_loss:.4f}\\t\\t\\t Validation Loss:{train_loss:.4f}')\n    if min_valid_loss > valid_loss:\n        print(f'Validation Loss Decreased From {min_valid_loss}----->{valid_loss}     ....Saving Model')\n        torch.save(model.state_dict(), PATH)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T00:29:29.206615Z","iopub.execute_input":"2021-11-19T00:29:29.20715Z","iopub.status.idle":"2021-11-19T00:29:33.247338Z","shell.execute_reply.started":"2021-11-19T00:29:29.207112Z","shell.execute_reply":"2021-11-19T00:29:33.244591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# GPT2 and VIT","metadata":{}},{"cell_type":"code","source":"from transformers import BertConfig, ViTConfig, VisionEncoderDecoderConfig, VisionEncoderDecoderModel","metadata":{"execution":{"iopub.status.busy":"2021-11-19T01:43:50.907006Z","iopub.execute_input":"2021-11-19T01:43:50.90775Z","iopub.status.idle":"2021-11-19T01:43:50.926576Z","shell.execute_reply.started":"2021-11-19T01:43:50.907686Z","shell.execute_reply":"2021-11-19T01:43:50.92575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CaptionDataset:\n    def __init__(self, df, tranform):\n        self.df = df\n        self.transform = tranform\n        \n        self.tokenizer = Tokenizer(32)\n        self.tokenizer.build_vocab(df['text'])\n    \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, idx):\n        img_path = self.df.loc[idx, 'images']\n        \n        img = Image.open('../input/flickr8k/Images/' + img_path)\n        img = self.transform(img)\n        \n        caption = self.tokenizer(self.df.loc[idx, 'text'])\n        \n        return {\n            'image': torch.tensor(img),\n            'text': torch.tensor(caption, dtype = torch.long)\n        }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":">>> from transformers import VisionEncoderDecoderModel\n>>> # initialize a vit-bert from a pretrained ViT and a pretrained BERT model. Note that the cross-attention layers will be randomly initialized\n>>> model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained('google/vit-base-patch16-224-in21k', 'bert-base-uncased')\n>>> # saving model after fine-tuning\n>>> model.save_pretrained(\"./vit-bert\")\n>>> # load fine-tuned model\n>>> model = VisionEncoderDecoderModel.from_pretrained(\"./vit-bert\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}