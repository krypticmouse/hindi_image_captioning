{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install git+https://github.com/huggingface/transformers.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wget https://raw.githubusercontent.com/rathiankit03/ImageCaptionHindi/master/Flickr8kHindiDataset/Flickr8k-Hindi.txt","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import transformers\nprint(transformers.__version__)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T04:27:35.988351Z","iopub.execute_input":"2021-11-19T04:27:35.988674Z","iopub.status.idle":"2021-11-19T04:27:37.398046Z","shell.execute_reply.started":"2021-11-19T04:27:35.988591Z","shell.execute_reply":"2021-11-19T04:27:37.396318Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nbase_path = '../input/flickr8k/Images/'\nwith open('./Flickr8k-Hindi.txt') as f:\n    data = []\n    \n    for i in f.readlines():\n        sp = i.split(' ')\n        data.append([sp[0] + '.jpg', ' '.join(sp[1:])])\n        \nhindi = pd.DataFrame(data, columns = ['images', 'text'])\n#hindi['images'] = hindi['images']!='2258277193_58694969e2'\nhindi.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-19T05:03:44.566881Z","iopub.execute_input":"2021-11-19T05:03:44.567588Z","iopub.status.idle":"2021-11-19T05:03:44.708567Z","shell.execute_reply.started":"2021-11-19T05:03:44.567549Z","shell.execute_reply":"2021-11-19T05:03:44.707884Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"hindi = hindi[hindi['images']!='2258277193_58694969e2']\nhindi","metadata":{"execution":{"iopub.status.busy":"2021-11-19T05:05:27.531248Z","iopub.execute_input":"2021-11-19T05:05:27.531721Z","iopub.status.idle":"2021-11-19T05:05:27.554041Z","shell.execute_reply.started":"2021-11-19T05:05:27.531681Z","shell.execute_reply":"2021-11-19T05:05:27.553342Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(hindi, test_size=0.2)\n# we reset the indices to start from zero\ntrain_df.reset_index(drop=True, inplace=True)\ntest_df.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T05:05:33.589475Z","iopub.execute_input":"2021-11-19T05:05:33.589935Z","iopub.status.idle":"2021-11-19T05:05:33.601823Z","shell.execute_reply.started":"2021-11-19T05:05:33.589891Z","shell.execute_reply":"2021-11-19T05:05:33.601024Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nclass Image_Caption_Dataset(Dataset):\n    def __init__(self,root_dir,df, feature_extractor,tokenizer,max_target_length=128):\n        self.root_dir = root_dir\n        self.df = df\n        self.feature_extractor = feature_extractor\n        self.tokenizer = tokenizer\n        self.max_length=max_target_length\n        \n    def __len__(self,df):\n        return self.df.shape[0]\n    \n    def __getitem__(self,idx):\n        #return image\n        image_path = self.df['images'][idx]\n        text = self.df['text'][idx]\n        #prepare image\n        image = Image.open(self.root_dir+'/'+image_path).convert(\"RGB\")\n        pixel_values = self.feature_extractor(image, return_tensors=\"pt\").pixel_values\n        #add captions by encoding the input\n        captions = self.tokenizer(text,\n                                 padding='max_length',\n                                 max_length=self.max_length).input_ids\n        captions = [caption if caption != self.tokenizer.pad_token_id else -100 for caption in captions]\n        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(captions)}\n        return encoding","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import ViTFeatureExtractor,AutoTokenizer\n\nencoder_checkpoint = 'google/vit-base-patch16-224'\ndecoder_checkpoint = 'surajp/gpt2-hindi'\n\nfeature_extractor = ViTFeatureExtractor.from_pretrained(encoder_checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(decoder_checkpoint)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"root_dir = \"../input/flickr8k/Images\"\n\n\ntrain_dataset = Image_Caption_Dataset(root_dir=root_dir,\n                           df=train_df,\n                           feature_extractor=feature_extractor,\n                                     tokenizer=tokenizer)\nval_dataset = Image_Caption_Dataset(root_dir=root_dir,\n                           df=test_df,\n                           feature_extractor=feature_extractor,\n                                     tokenizer=tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoding = train_dataset[0]\nfor k,v in encoding.items():\n  print(k, v.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = encoding['labels']\nlabels[labels == -100] = tokenizer.pad_token_id\nlabel_str = tokenizer.decode(labels, skip_special_tokens=True)\nprint(label_str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\neval_dataloader = DataLoader(val_dataset, batch_size=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import VisionEncoderDecoderModel\n# initialize a vit-bert from a pretrained ViT and a pretrained BERT model. Note that the cross-attention layers will be randomly initialized\nmodel = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_checkpoint, decoder_checkpoint)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-11-19T05:18:55.839835Z","iopub.execute_input":"2021-11-19T05:18:55.840280Z","iopub.status.idle":"2021-11-19T05:19:05.132280Z","shell.execute_reply.started":"2021-11-19T05:18:55.840244Z","shell.execute_reply":"2021-11-19T05:19:05.131549Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# set special tokens used for creating the decoder_input_ids from the labels\nmodel.config.decoder_start_token_id = tokenizer.cls_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\n# make sure vocab size is set correctly\nmodel.config.vocab_size = model.config.decoder.vocab_size\n\n# set beam search parameters\nmodel.config.eos_token_id = tokenizer.sep_token_id\nmodel.config.max_length = 128\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4","metadata":{"execution":{"iopub.status.busy":"2021-11-19T05:12:26.304665Z","iopub.execute_input":"2021-11-19T05:12:26.305061Z","iopub.status.idle":"2021-11-19T05:12:26.310599Z","shell.execute_reply.started":"2021-11-19T05:12:26.305021Z","shell.execute_reply":"2021-11-19T05:12:26.309695Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"from datasets import load_metric\n\nbleu_metric = load_metric(\"bleu\")","metadata":{"execution":{"iopub.status.busy":"2021-11-19T05:20:41.695552Z","iopub.execute_input":"2021-11-19T05:20:41.696287Z","iopub.status.idle":"2021-11-19T05:20:42.690597Z","shell.execute_reply.started":"2021-11-19T05:20:41.696236Z","shell.execute_reply":"2021-11-19T05:20:42.689856Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n\n    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n    labels_ids[labels_ids == -100] = tokenizer.tokenizer.pad_token_id\n    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n\n    bleu = bleu_metric.compute(predictions=pred_str, references=label_str)\n\n    return {\"bleu\": bleu}","metadata":{"execution":{"iopub.status.busy":"2021-11-19T05:20:37.837354Z","iopub.execute_input":"2021-11-19T05:20:37.837652Z","iopub.status.idle":"2021-11-19T05:20:37.844275Z","shell.execute_reply.started":"2021-11-19T05:20:37.837620Z","shell.execute_reply":"2021-11-19T05:20:37.843394Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW\nfrom tqdm.notebook import tqdm\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW\nfrom tqdm.notebook import tqdm\n\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\nfor epoch in range(2):  # loop over the dataset multiple times\n   # train\n   model.train()\n   train_loss = 0.0\n   for batch in tqdm(train_dataloader):\n      # get the inputs\n      for k,v in batch.items():\n        batch[k] = v.to(device)\n\n      # forward + backward + optimize\n      outputs = model(**batch)\n      loss = outputs.loss\n      loss.backward()\n      optimizer.step()\n      optimizer.zero_grad()\n\n      train_loss += loss.item()\n\n   print(f\"Loss after epoch {epoch}:\", train_loss/len(train_dataloader))\n    \n   # evaluate\n   model.eval()\n   valid_bleu = 0.0\n   with torch.no_grad():\n     for batch in tqdm(eval_dataloader):\n       # run batch generation\n       outputs = model.generate(batch[\"pixel_values\"].to(device))\n       # compute metrics\n       cer = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])\n       valid_bleu += cer \n\n   print(\"Validation BLEU:\", valid_bleu / len(eval_dataloader))\n\nmodel.save_pretrained(\".\")","metadata":{"execution":{"iopub.status.busy":"2021-11-19T05:15:13.308344Z","iopub.execute_input":"2021-11-19T05:15:13.308599Z","iopub.status.idle":"2021-11-19T05:15:13.314193Z","shell.execute_reply.started":"2021-11-19T05:15:13.308570Z","shell.execute_reply":"2021-11-19T05:15:13.313241Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2021-11-19T05:18:09.421680Z","iopub.execute_input":"2021-11-19T05:18:09.422272Z","iopub.status.idle":"2021-11-19T05:18:09.426510Z","shell.execute_reply.started":"2021-11-19T05:18:09.422234Z","shell.execute_reply":"2021-11-19T05:18:09.425619Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}